import requests
import datetime
import io
import pandas as pd
import time
import random
import concurrent.futures
from Alpha_Finance.data import DataReader


def crawl_price(date):
    
    # å°‡jsonæ”¹æˆcsv ---refer to FinLabéƒ¨è½æ ¼
    url = f"https://www.twse.com.tw/exchangeReport/MI_INDEX?response=csv&date={date.strftime('%Y%m%d')}&type=ALLBUT0999&_=1629263883842"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.22 (KHTML, like Gecko) Chrome/92.0.1234.567 Safari/111.11'}
    res = requests.get(url,headers=headers)
    
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12]).replace('=','')
    df_sii = pd.read_csv(io.StringIO(data)).rename(columns={'è­‰åˆ¸ä»£è™Ÿ':'stock_id'}).set_index('stock_id')[['é–‹ç›¤åƒ¹','æ”¶ç›¤åƒ¹','æœ€é«˜åƒ¹','æœ€ä½åƒ¹','æˆäº¤è‚¡æ•¸']]
    df_sii = df_sii.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')
    df_sii['date'] = pd.to_datetime(date)
    df_sii = df_sii.loc[[sid for sid in df_sii.index if len(sid)==4]]
    
    url = f"https://www.tpex.org.tw/web/stock/aftertrading/otc_quotes_no1430/stk_wn1430_result.php?l=zh-tw&o=csv&d={str(date.year-1911)+'/'+date.strftime('%m/%d')}&se=EW&s=0,asc,0"
    res = requests.get(url,headers=headers)
    
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12])
    df_otc = pd.read_csv(io.StringIO(data))
    df_otc.columns = res.text.split('\n')[3].replace(' ','').split(',')
    df_otc = df_otc.rename(columns={'ä»£è™Ÿ':'stock_id','æ”¶ç›¤':'æ”¶ç›¤åƒ¹','é–‹ç›¤':'é–‹ç›¤åƒ¹','æœ€é«˜':'æœ€é«˜åƒ¹','æœ€ä½':'æœ€ä½åƒ¹'}).set_index('stock_id')[['é–‹ç›¤åƒ¹','æ”¶ç›¤åƒ¹','æœ€é«˜åƒ¹','æœ€ä½åƒ¹','æˆäº¤è‚¡æ•¸']]
    df_otc = df_otc.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna()
    df_otc['date'] = pd.to_datetime(date)
    df_otc = df_otc.loc[[sid for sid in df_otc.index if len(sid)==4]]
    
    time.sleep(random.randint(5,7))
    return df_sii.append(df_otc)



def crawl_price_multithreading(dates):
    
    print('========= Start crawling price =========')
    database = pd.DataFrame()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        data = {executor.submit(crawl_price,date):date for date in dates}
        for datum in concurrent.futures.as_completed(data):
            try:
                df = datum.result()
                database = database.append(df)
                print(df['date'].iloc[-1].strftime('%Y-%m-%d'),'ğŸ†—')
            except:
                pass
    
    try:
        database = database.sort_values('date')
        df = DataReader.get_twstock_close().append(database.pivot_table('æ”¶ç›¤åƒ¹','date','stock_id'))
        df.groupby(df.index).last()
        df.to_pickle('DB_twstock/Close.pkl')
        df = DataReader.get_twstock_open().append(database.pivot_table('é–‹ç›¤åƒ¹','date','stock_id'))
        df.groupby(df.index).last()
        df.to_pickle('DB_twstock/Open.pkl')
        df = DataReader.get_twstock_high().append(database.pivot_table('æœ€é«˜åƒ¹','date','stock_id'))
        df.groupby(df.index).last()
        df.to_pickle('DB_twstock/High.pkl')
        df = DataReader.get_twstock_low().append(database.pivot_table('æœ€ä½åƒ¹','date','stock_id'))
        df.groupby(df.index).last()
        df.to_pickle('DB_twstock/Low.pkl')
        df = DataReader.get_twstock_volume().append(database.pivot_table('æˆäº¤è‚¡æ•¸','date','stock_id'))
        df.groupby(df.index).last()
        df.to_pickle('DB_twstock/Volume.pkl')
    except:
        pass
    print('=========       Finishâœ”         =========')
    
    
    
def crawl_institution(date):
    
    url = f"https://www.twse.com.tw/fund/T86?response=csv&date={date.strftime('%Y%m%d')}&selectType=ALLBUT0999&_=1629339983742"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.22 (KHTML, like Gecko) Chrome/92.0.1234.567 Safari/111.11'}
    res = requests.get(url,headers=headers)
    
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12])
    df_sii = pd.read_csv(io.StringIO(data.replace('=',''))).rename(columns={'è­‰åˆ¸ä»£è™Ÿ':'stock_id'}).set_index('stock_id')
    df_sii = df_sii.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')[['å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)','å¤–è³‡è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸','æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸','è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸']]
    df_sii['date'] = pd.to_datetime(date)
    df_sii = df_sii.loc[[sid for sid in df_sii.index if len(sid)==4]]

    url = f"https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php?l=zh-tw&o=csv&se=EW&t=D&d={str(date.year-1911)+'/'+date.strftime('%m/%d')}&s=0,asc"
    res = requests.get(url,headers=headers)
    
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12])
    df_otc = pd.read_csv(io.StringIO(data))
    df_otc.columns = res.text.split('\n')[1].replace(' ','').replace('-','').split(',')
    df_otc = df_otc.rename(columns={'ä»£è™Ÿ':'stock_id','å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)è²·è³£è¶…è‚¡æ•¸':'å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)'}).set_index('stock_id')
    df_otc = df_otc.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')[['å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)','å¤–è³‡è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸','æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸','è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸']]
    df_otc['date'] = pd.to_datetime(date)
    df_otc = df_otc.loc[[sid for sid in df_otc.index if len(sid)==4]]
    
    time.sleep(random.randint(5,7))
    return df_sii.append(df_otc)


def crawl_institution_multithreading(dates):
    
    print('========= Start crawling institution =========')
    database = pd.DataFrame()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        data = {executor.submit(crawl_institution,date):date for date in dates}
        for datum in concurrent.futures.as_completed(data):
            try:
                df = datum.result()
                database = database.append(df)
                print(df['date'].iloc[-1].strftime('%Y-%m-%d'),'ğŸ†—')
            except:
                pass
                
    try:
        database = database.sort_values('date')
        database['å¤–è³‡è²·è³£è¶…è‚¡æ•¸'] = database[['å¤–é™¸è³‡è²·è³£è¶…è‚¡æ•¸(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)','å¤–è³‡è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸']].sum(axis=1)
        df = DataReader.get_twstock_foreign_investors().append(database.pivot_table('å¤–è³‡è²·è³£è¶…è‚¡æ•¸','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/foreign_investors.pkl')
        df = DataReader.get_twstock_investment_trust().append(database.pivot_table('æŠ•ä¿¡è²·è³£è¶…è‚¡æ•¸','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/investment_trust.pkl')
        df = DataReader.get_twstock_dealer().append(database.pivot_table('è‡ªç‡Ÿå•†è²·è³£è¶…è‚¡æ•¸','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/dealer.pkl')
    except:
        pass
    print('=========       Finishâœ”         =========')
    
    
    
def crawl_margin(date):
    
    url = f"https://www.twse.com.tw/exchangeReport/MI_MARGN?response=csv&date={date.strftime('%Y%m%d')}&selectType=ALL&_=1629348061458"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.22 (KHTML, like Gecko) Chrome/92.0.1234.567 Safari/111.11'}
    res = requests.get(url,headers)
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12])
    df_sii = pd.read_csv(io.StringIO(data.replace('=',''))).rename(columns={'è‚¡ç¥¨ä»£è™Ÿ':'stock_id'}).set_index('stock_id')
    df_sii = df_sii.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')
    df_sii['èè³‡è²·è¶…è‚¡æ•¸'] = df_sii['è²·é€²']-df_sii['è³£å‡º']
    df_sii['èåˆ¸è³£è¶…è‚¡æ•¸'] = df_sii['è³£å‡º.1']-df_sii['è²·é€².1']
    df_sii['date'] = pd.to_datetime(date)
    df_sii = df_sii[['èè³‡è²·è¶…è‚¡æ•¸','èåˆ¸è³£è¶…è‚¡æ•¸','date']].loc[[sid for sid in df_sii.index if len(sid)==4]]
    
    url = f"https://www.tpex.org.tw/web/stock/margin_trading/margin_balance/margin_bal_result.php?l=zh-tw&o=csv&d={str(date.year-1911)+'/'+date.strftime('%m/%d')}&s=0,asc"
    res = requests.get(url,headers=headers)
    data = '\n'.join([l for l in res.text.split('\n') if len(l.split(',"'))>=12])
    df_otc = pd.read_csv(io.StringIO(data))
    df_otc.columns = res.text.split('\n')[2].replace(' ','').split(',')
    df_otc = df_otc.rename(columns={'ä»£è™Ÿ':'stock_id'}).set_index('stock_id').applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')
    df_otc['èè³‡è²·è¶…è‚¡æ•¸'] = df_otc['è³‡è²·']-df_otc['è³‡è³£']
    df_otc['èåˆ¸è³£è¶…è‚¡æ•¸'] = df_otc['åˆ¸è³£']-df_otc['åˆ¸è²·']
    df_otc['date'] = pd.to_datetime(date)
    df_otc = df_otc[['èè³‡è²·è¶…è‚¡æ•¸','èåˆ¸è³£è¶…è‚¡æ•¸','date']].loc[[sid for sid in df_otc.index if len(sid)==4]]
    
    time.sleep(random.randint(5,7))
    return df_sii.append(df_otc)


def crawl_margin_multithreading(dates):
    
    print('========= Start crawling margin =========')
    database = pd.DataFrame()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        data = {executor.submit(crawl_margin,date):date for date in dates}
        for datum in concurrent.futures.as_completed(data):
            try:
                df = datum.result()
                database = database.append(df)
                print(df['date'].iloc[-1].strftime('%Y-%m-%d'),'ğŸ†—')
            except:
                pass
    try:        
        database = database.sort_values('date')
        df = DataReader.get_twstock_margin_trading().append(database.pivot_table('èè³‡è²·è¶…è‚¡æ•¸','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/margin_trading.pkl')
        df = DataReader.get_twstock_short_selling().append(database.pivot_table('èåˆ¸è³£è¶…è‚¡æ•¸','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/short_selling.pkl')
    except:
        pass
    print('=========       Finishâœ”         =========')
    
    
    
    
def crawl_monthly_revenue(month):
    
    if month.month==1:
        month_str = str(month.year-1912)+'_'+'12'
    else:
        month_str = str(month.year-1911)+'_'+str(month.month-1)
    
    url = 'https://mops.twse.com.tw/server-java/FileDownLoad'
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/555.22 (KHTML, like Gecko) Chrome/92.0.1234.567 Safari/111.11'}
    payload = {
        'step': '9',
        'functionName': 'show_file',
        'filePath': '/home/html/nas/t21/sii/',
        'fileName': f"t21sc03_{month_str}.csv"
    }
    res = requests.post(url,headers=headers,data=payload)
    res.encoding = 'utf-8'
    df_sii = pd.read_csv(io.StringIO(res.text)).rename(columns={'å…¬å¸ä»£è™Ÿ':'stock_id','ç‡Ÿæ¥­æ”¶å…¥-ç•¶æœˆç‡Ÿæ”¶':'ç•¶æœˆç‡Ÿæ”¶'}).set_index('stock_id')
    df_sii = df_sii.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')
    df_sii['date'] = pd.to_datetime(month.strftime('%Y-%m')+'-10')
    
    time.sleep(7)

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 20.0; Win64; x64) AppleWebKit/152.52 (KHTML, like Gecko) Chrome/93.0.1459.321 Safari/234.01'}
    payload = {
        'step': '9',
        'functionName': 'show_file',
        'filePath': '/home/html/nas/t21/otc/',
        'fileName': f"t21sc03_{month_str}.csv"
    }
    res = requests.post(url,headers=headers,data=payload)
    res.encoding = 'utf-8'
    df_otc = pd.read_csv(io.StringIO(res.text)).rename(columns={'å…¬å¸ä»£è™Ÿ':'stock_id','ç‡Ÿæ¥­æ”¶å…¥-ç•¶æœˆç‡Ÿæ”¶':'ç•¶æœˆç‡Ÿæ”¶'}).set_index('stock_id')
    df_otc = df_otc.applymap(lambda s:pd.to_numeric(str(s).replace(',',''),errors='coerce')).dropna(axis=1,how='all')
    df_otc['date'] = pd.to_datetime(month.strftime('%Y-%m')+'-10')
    time.sleep(8)

    return df_sii.append(df_otc)[['ç•¶æœˆç‡Ÿæ”¶','date']]


def crawl_monthly_revenue_multithreading(months):
    
    print('========= Start crawling monthly_revenue =========')
    database = pd.DataFrame()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        data = {executor.submit(crawl_monthly_revenue,month):month for month in months}
        for datum in concurrent.futures.as_completed(data):
            try:
                df = datum.result()
                database = database.append(df)
                print(df['date'].iloc[-1].strftime('%Y-%m'),'ğŸ†—')
            except:
                print('æŸ¥è©¢éæ–¼é »ç¹,è«‹ç¨å¾Œå†è©¦!')
                
    try:
        database = database.sort_values('date')
        database.index = database.index.astype(str)

        df = DataReader.get_twstock_monthly_revenue().append(database.pivot_table('ç•¶æœˆç‡Ÿæ”¶','date','stock_id'))
        df = df.groupby(df.index).last()
        df.to_pickle('DB_twstock/monthly_revenue.pkl')
    except:
        pass
    print('=========       Finishâœ”         =========')
    
    

def auto_crawler():
    
    start_date = DataReader.get_twstock_close().index[-1]+datetime.timedelta(1)
    end_date = datetime.datetime.now()
    dates = pd.date_range(start_date,end_date)
    crawl_price_multithreading(dates)
    
    start_date = DataReader.get_twstock_foreign_investors().index[-1]+datetime.timedelta(1)
    end_date = datetime.datetime.now()
    dates = pd.date_range(start_date,end_date)
    crawl_institution_multithreading(dates)
    
    start_date = DataReader.get_twstock_margin_trading().index[-1]+datetime.timedelta(1)
    end_date = datetime.datetime.now()
    dates = pd.date_range(start_date,end_date)
    crawl_margin_multithreading(dates)
    
    start_date = DataReader.get_twstock_monthly_revenue().index[-1]
    end_date = datetime.datetime.now()
    dates = pd.date_range(start_date,end_date)
    months = pd.to_datetime(sorted(list(set(dates.strftime('%Y-%m')))))
    crawl_monthly_revenue_multithreading(months)
    
    print('â­â­â­â­â­ All Finish â­â­â­â­â­')